# ipc-bench

## 简介:

本项目希望通过编写bench程序, 使用不同的方法来进行程序间的通信, 设定benchmark来测定不同进程间通信方法的性能的区别。

项目地址: https://github.com/OS-F-4/ipc-bench/tree/linux-rfc-v1

## 运行方式:

在根文件夹下:

```shell
mkdir build
cd build
cmake ..
make
```

在`build/source`会出现对应ipc方法的文件夹, 文件夹中有可执行文件。如执行`build/source//tcp/tcp -c 200  -s 5`  代表使用tcp方法, 收发200次, 每次的大小为5个字节。

程序会有类似如下的输出:

```
[root@localhost source]# ./uintr-shm/uintr-shm -s 1
============ RESULTS ================
Message size:       1
Message count:      1000
Total duration:     2.648	ms
Average duration:   1.934	us
Minimum duration:   1.792	us
Maximum duration:   71.680	us
Standard deviation: 2.210	us
Message rate:       377670	msg/s
=====================================
```

 其中`uintrfd-bi`是两个线程来回发送消息。 `uintrfd-uni`是两个线程一发一收, 单向发送。`uintr-sm-bi`是两个线程公用同一块内存进行更大的消息传递。`uint-shm` 是两个进程通过共享内存的方式进行更大的消息的传递。



##  qemu结果

| IPC type        | Message rate  (msg/s, size = 1) | Message rate  (msg/s, size = 4096) |
| --------------- | ------------------------------- | ---------------------------------- |
| uintr (thread)  | 28935                           | 23305                              |
| uintr (process) | 34392                           | 22662                              |
| signal          | 6286                            | N/A                                |
| eventfd         | 6649                            | N/A                                |
| pipe            | 2858                            | 2256                               |
| fifo            | 2782                            | 2674                               |
| domain          | 4835                            | 2781                               |



## 物理机器结果

### size = 1

| method      | Average(us) | min(us) | max(us) | rate(msg/s) |
| ----------- | ----------- | ------- | ------- | ----------- |
| eventfd-bi  | 37.250      | 11.520  | 274.944 | 26630       |
| mq          | 126.553     | 19.968  | 380.672 | 7841        |
| shm         | 1.568       | 1.280   | 11.264  | 503577      |
| signal      | 97.651      | 7.936   | 326.144 | 10159       |
| uintrfd-bi  | 4.366       | 4.096   | 5.632   | 191868      |
| uintrfd-uni | 2.035       | 1.792   | 3.584   | 280440      |
| domain      | 88.644      | 16.640  | 342.016 | 11183       |
| fifo        | 38.607      | 22.528  | 364.800 | 25544       |
| mmap        | 1.314       | 1.024   | 11.008  | 574026      |
| pipe        | 105.078     | 25.088  | 339.456 | 9436        |
| tcp         | 170.654     | 36.864  | 367.104 | 5828        |
| uintr-shm   | 1.934       | 1.792   | 71.680  | 377670      |



### size = 4096

| method        | Average(us) | min(us) | max(us) | rate(msg/s) |
| ------------- | ----------- | ------- | ------- | ----------- |
| shm           | 7.034       | 6.400   | 88.320  | 133986      |
| uintrfd-sm-bi | 7.744       | 7.168   | 44.032  | 116340      |
| domain        | 50.049      | 21.760  | 276.992 | 19758       |
| fifo          | 59.966      | 23.808  | 280.576 | 16527       |
| mmap          | 6.660       | 6.400   | 15.872  | 141050      |
| pipe          | 70.535      | 26.624  | 296.960 | 14011       |
| tcp           | 162.982     | 39.168  | 423.424 | 6098        |
| uintr-shm     | 2.173       | 2.048   | 74.240  | 347037      |



## 结果分析:

我们可以看到, 基于用户态中断方法的进程间通信性能, 相比所有的直接通知的IPC机制都要高效, 可以获得**数十倍的性能提升**。对于`shm`和`mmap`而言, 其实现为两边用户轮询一块共享的内存, 所以延迟和速度比较低, 但是这对cpu的占用较高, cpu无法进行其他任务, 对于真正的应用而言, 这样的进程间通信方式是不实际的。

对比qemu和物理机的数据, 同样条件下, qemu的运行速度慢于物理机, 这符合模拟器运行速度慢的直觉。相同执行环境中不同方法之间的相对差距幅度也比较相似, 这也说明了qemu对硬件的性质的模拟是比较到位的, 同时也验证了我们在qemu中实现的用户态中断支持是有效且到位的。
