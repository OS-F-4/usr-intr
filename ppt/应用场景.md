---
marp: true
---

## 进展与应用场景设计

**设计进展**

* 解决一个问题：链接数量
* 发现新问题：链式调用，load-balance

**应用场景**

* xpc / *bridge 的用户测例

---

## 设计进展

* 链接数量
  * 基础：64
  * 需求：512 (skybridge) ?
  * 编号复用

---

### 与 *bridge 的相同

*bridge 的优势 （与已有同步 IPC 相比）

* context switch 小
* 链接一但建立，通信的代码路径很短

核心：专用通信通道，bypass kernel

|            操作            |   cycles   |
| :------------------------: | :--------: |
|         func call          |     24     |
|        write to CR3        |    186     |
| no-op system call w/ KPTI  |    431     |
| no-op system call w/o KPTI |    181     |
|           VMFUNC           |    134     |
|           wrpkru           |     28     |
|           xcall            |  150 ~ 21  |
|           uintr            | ??? (~150) |



![bg right contain vertical](sky1.jpg)

---

### 与 *bridge 的区别

同步  vs  异步

异步的缺点：

* 延迟不好控制：链式调用

  * eg: 应用 -> 数据库  ->  文件系统服务 -> 磁盘驱动服务
  * 问题：所有服务都需要在线，否则整体延时崩了
* 需要 load-balance 
  * 平衡 sender / receiver 的 load


异步优势: 吞吐量
  * 专核专用 -> 核多多益善
    *  (同步性能与核数关系不大)
  * 最底层硬件是异步的：硬件中断

---

### *bridge 测试

* skybridge / underbridge
  * IPC ping-pong: 无负载平均延时
  * KVS: 有负载的简单测试
  * 微内核 sqlite3 + xv6fs + ram-disk: 真实 app
* XPC
  * IPC ping-pong: 无负载平均延时
  * 微内核 fs / TCP: 有负载的简单测试
  * 微内核 sqlite3 / 微内核 android binder / 微内核 TCP HTTP server: 真实 app


---

### uintr 的优势

* 安全：不会破坏隔离性
* 对内核修改小 ？？？

---

### 应用场景：难点

最理想：sender / receiver / driver(syscall) 全异步 / 全在线 / 负载均衡

全异步 （工作量）

* 同步 sender: 线程库，尽力挖掘异步潜能
* receiver: 基于事件机制，适配比较容易
* driver / syscall: 需要一层异步封装

全在线 （设计专门场景，通用性？）

* 设计针对的 OS 调度
  * 如何尽量驻留：尽力多核，专核专用
  * 如何快速唤醒：新调度类（低于 RT，高于 CFS) ?
  * 如何切换状态：自适应

负载均衡 （设计专门场景，难。。。）

* 同步 receiver 运行状态:
  *  // todo

* receiver 负载不够，减少核数

* sender 负载不够 ==> 增加负载？需要切换？
  * 批量提交请求？
  * 轮询？

通用性

* 简单、阻塞的 IPC 适合异步吗？
* 如何兼容同步？

---

### 应用场景：设计

* sender 有一定工作负载，异步
* 链路层数不多：容易满足
* 终点 server 异步/高速：DPDK





